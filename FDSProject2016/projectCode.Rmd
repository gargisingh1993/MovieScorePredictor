---
title: Are Facebook likes, profits and movie reviews accurate predictors for IMDB
  score?
author: "kvk229 & gsc326"
date: "11/22/2016"
output: html_document
---


# GAM model general additive model
# log linear regression
<!-- https://rstudio-pubs-static.s3.amazonaws.com/33876_1d7794d9a86647ca90c4f182df93f0e8.html -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE, include=FALSE}
# setting up the libraries
# require(caTools)
#install.packages('xlsx')
library(gdata)
library(car)
library(caret)
library(caTools)
library(ggbiplot,vqv)
library(randomForest)
#library(xlsx)
```

```{r}
# setting the working directory
setwd('/Users/kkiran/Desktop/fall_2016/fds/project/MovieScorePredictor/data')
# 'C:/Users/gogs/Documents/GitHub/MovieScorePredictor/Data'
# setwd('C:/Users/gogs/Documents/GitHub/MovieScorePredictor/Data')
movieData = read.xls("movie_data.xls")
#head(movieData)
```

Loaded the data into a data frame 'movieData'

```{r}
# identifying top 10 genres out of all the 26 genres to make the work more focussed

# gernes present in the data:
# 'Sci-Fi', 'Crime', Romance', Animation', Music', Comedy', War', genres', Horror', Film-Noir', Adventure', News', Reality-TV', Thriller', Western', Mystery', Short', Drama', Action', Documentary', Musical', History', Family', Fantasy', Game-Show', Sport', Biography'

movieCount <- c()

for(i in 38:64)
{
  movieCount[i - 37] = sum(movieData[,i]);
}
movieCount

genreNames <- as.vector(colnames(movieData)[38:64])
genreNames

genreNames <- as.vector(genreNames)

genreMovieCount <- data.frame(genreNames, movieCount)
plot(genreMovieCount$genreNames, genreMovieCount$movieCount, main="Genre Distribution",	xlab="Genre ", ylab=" Movies Made")
```

We can see that not all the genres have a considerable number of movies made in them, so we decided to extract the top 11 genres that have the most number of movies made in those particular genres.

```{r}
genreMovieCountSorted <- genreMovieCount[order(-movieCount),] 
genreMovieCountSorted <- genreMovieCountSorted[c(1:10),]
plot(genreMovieCountSorted$genreNames, genreMovieCountSorted$movieCount, main="Filtered Genre Distribution",	xlab="Genre ", ylab=" Movies Made")

```

Now we have to remove the data for all the other genres from the data set, also we can delete the last column from the data set because it is repeated
```{r}

movieData <- movieData[,-65]
columnNames <- colnames(movieData)
columnNames <- columnNames[1:37]

selectedNames <- genreMovieCountSorted$genreNames

columnNames <- as.vector(columnNames)
selectedNames <- as.vector(selectedNames)

names <- c(columnNames, selectedNames)
names

movieData1 <- subset(movieData, select = names)
# for(i in 38:47)
# {
#   print(c(colnames(movieData1[i]), sum(movieData1[,i])));
# }
```
Now movieData1 has the data about the genres that we are interested only. The next step is to clean the data by removing rows that dont have a considerable amount of data. If the facebook likes are missing, although we can get those details from other row, we are not proceeding so because the number of facebook likes are always changing and fetching data from other rows might not be a very good estimate for plugging in missing likes data.

Note: The facebook data is present in the data itself, we didnt have to fetch the data manually.

```{r}

mean(is.na(movieData))
paste("only", mean(is.na(movieData)), " (mean) amount of data is null, so we can safely remove NAs")

row.has.na <- apply(movieData1, 1, function(x){any(is.na(x))})
numberOfNAs <- sum(row.has.na)

print (c("can remove ", numberOfNAs , "null rows from the table"))
#removing the nulls
movieData1 <- na.omit(movieData1)
```

```{r}
NAcounter <- 0
indicesToRemove <- c()
index <- 1
# this is the working version 
for (i in 1 : nrow(movieData1)) {
  if (any(movieData1[i,] == "N/A")) {
     # print (c(i, "yes" , movieData1[i,]))
    indicesToRemove[index] = i;
    index <- index + 1
    NAcounter <- NAcounter + 1
  } 
    # print ("no")
}

print(length(indicesToRemove))
# print(indicesToRemove)
print(c("total number of nulls", NAcounter))
```

```{r}
#removing the '@NAcounter' number of rows that have NA in them
movieData2 <- movieData1[-indicesToRemove,]
# Now, movieData2 has no NA in any of the rows.

movieData2[movieData2==""] <- NA
row.has.na <- apply(movieData2, 1, function(x){any(is.na(x))})
numberOfNAs <- sum(row.has.na)
paste("There are ",numberOfNAs, " rows with empty cells, so we are removing them")
# removing the empty rows, (second round of filtering)
movieData2 <- na.omit(movieData2)
```


```{r}
#couting the profits of a movie by subtracting the budget from the gross
movieData2$profits <- movieData2$gross - movieData2$budget
movieData3 <- movieData2[,c(c(1:13),48, c(14:47))]

# movieData3 is the final cleaned data that also has a column showing the profits made by the movie
#str(movieData3)
stat <- nearZeroVar(movieData3, saveMetrics = T)
class(stat$zeroVar)
varDF <- cbind.data.frame(colnames(movieData3),stat$zeroVar)

#converting logical to binary
cols <- sapply(varDF, is.logical)
varDF[,cols] <- lapply(varDF[,cols], as.numeric)

# for ( i in 1:nrow(varDF)) {
#  print(varDF[i,2]) 
# }

plot(varDF$`colnames(movieData3)`, varDF$`stat$zeroVar`, xlab = "predictors", ylab = "Zero - variance", main = "zero variance scores of different possible predictors")

paste("we can see that all the columns have a zero score for the zero-variance score showing that all the columns can participate as a predictor in the prediction model")

# We can see that none of the varibles have zero variance, so we can consider all the variables for study
```


```{r}

#coverting factor to int array
movieData3$tomatoUserRating <- as.numeric(as.character(movieData3$tomatoUserRating))
movieData3$tomatoRating <- as.numeric(as.character(movieData3$tomatoRating))
movieData3$tomatoReviews <- as.numeric(as.character(movieData3$tomatoReviews))
movieData3$tomatoFresh <- as.numeric(as.character(movieData3$tomatoFresh))
movieData3$tomatoRotten <- as.numeric(as.character(movieData3$tomatoRotten))
movieData3$tomatoUserMeter <- as.numeric(as.character(movieData3$tomatoUserMeter))
movieData3$tomatoUserReviews <- as.numeric(as.character(movieData3$tomatoUserReviews))
movieData3$imdbVotes <- as.numeric(as.character(movieData3$imdbVotes))
movieData3$Metascore <- as.numeric(as.character(movieData3$Metascore))
```

```{r}
# checking the correlation
# str(movieData3)
cor(movieData3$imdb_score, movieData3[,c(7,12:14,16:26,38)], use = "pairwise.complete.obs")
colnames(movieData3)[c(7,12:14,16:26,38)]
```

```{r}
cNames <- paste("movieData3$",colnames(movieData3)[c(7,12:14,16:26,38)], sep = "")
# cNames
#formula contains all the columns that we want to include in the model
formula <- as.formula(paste("y ~ ", paste(cNames, collapse= "+")))
# formula
```


```{r}
#choice of linear regression vs logistic regression
# Linear regression: When the outcome(dependent variable) is continuous, i.e. infinite number of possibilities.
# Logistic regression: When the outcome(dependent variable) has a limited set of values.

#because we are trying to predict the IMDB score of a movie and theoritically the score can have an infinite number of possibilities, we are chosing linear regression over logisitic regression
#linear regression


lmfit1.movieData <- lm(movieData3$imdb_score ~ movieData3$cast_total_facebook_likes + movieData3$gross + movieData3$budget + movieData3$num_critic_for_reviews +movieData3$num_user_for_reviews + movieData3$tomatoUserRating + + movieData3$tomatoRating + movieData3$tomatoReviews + movieData3$tomatoFresh + movieData3$tomatoUserMeter + movieData3$tomatoUserReviews + movieData3$num_voted_users +  movieData3$imdbVotes + movieData3$duration ,data = movieData3)

# summary(lmfit1.movieData)

vif(lmfit1.movieData)
```
Looking at the Variance Inflation Factor of the fitted model, we can see that imdbVotes and num_voted_users are two largest VIFs. so we try to eliminate them and make the model again.

```{r}
# coefficients of multi variate linear regression
paste("coefficients of fitted line by linear regression")
# summary(lmfit1.movieData)$coefficients
```

```{r}
 lmfit2.movieData <- lm(movieData3$imdb_score ~ movieData3$cast_total_facebook_likes + movieData3$gross + movieData3$budget + movieData3$num_critic_for_reviews +movieData3$num_user_for_reviews + movieData3$tomatoUserRating + + movieData3$tomatoRating + movieData3$tomatoReviews + movieData3$tomatoFresh + movieData3$tomatoUserMeter + movieData3$tomatoUserReviews + movieData3$duration ,data = movieData3)

# summary(lmfit2.movieData)

lmfit2.movieData

vif(lmfit2.movieData)
# we can see that the std error for the parameter estimates gets smaller.
```

```{r}
#calculating the MSFE and MAD for the predicted values
predictedScore <- predict(lmfit2.movieData)
# predictedScore
RSFE_v <- movieData3$imdb_score - predictedScore
# RSFE_v
RSFE <- sum(RSFE_v)
# RSFE
absRSFE <- abs(RSFE)
# absRSFE
length(RSFE_v)
MSFE <- absRSFE / length(RSFE_v)
# MSFE
mean(lmfit2.movieData$residuals^2)


# calculating mad 
Madoriginal <- mad(movieData3$imdb_score,center = median(movieData3$imdb_score),constant = 1)
MadRegression <- mad(predictedScore, center = median(predictedScore), constant = 1)
```

```{r}
# Analysing the linear regression model
# log.movieData <- log(movieData3[, c(7,12:26,38)])
trans = preProcess(x = movieData3[,c(7,12:14,16:26,38)], method=c("BoxCox", "center", "scale", "pca"))
trans
trans$pcaComp


pca.movieData <- prcomp(x = movieData3[,c(7,12:14,16:26,38)], center = TRUE, scale. = TRUE) 
print(pca.movieData)
colnames(pca.movieData$rotation)
# summary(pca.movieData)
pca.movieData
plot(pca.movieData, type = "l", main = "PCA Movie Data")
```

```{r warning= FALSE}

wssplot <- function(data, nc=15, seed=1234) {
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  bss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc) {
    set.seed(seed)
    wss[i] <- sum(kmeans(data[,c(7,12:26,38)], centers=i)$withinss)
    bss[i] <- sum(kmeans(data[,c(7,12:26,38)], centers=i)$betweenss)
    }
  
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares", main = "chosing number of clusters")
  plot(1:nc, bss, type="b", xlab="Number of Clusters",
       ylab="Between groups sum of squares", main = "chosing number of clusters")
  }

wssplot(movieData3, nc=35)
paste("from both of the graphs above, we can see that 20 is the optimum number of clusters because the between groups sum of squares is remaining constant after 20 clusters and similarly the within groups sum of squares is also remaining constant after 20 clusters.")
paste("The within groups sum of squares was decreasing untill 20 clusters and the between groups sum of squares was increasing untill 20 clusters")
```

```{r}
set.seed(101) 
sample = sample.split(movieData3, SplitRatio = .75)
train = subset(movieData3, sample == TRUE)
test = subset(movieData3, sample == FALSE)
```

```{r}
#using Kmeans clustering

paste("The dimensions of the training data are")
dim(train)
paste("The dimensions of the testing data are")
dim(test)
k.means.fit <- kmeans(train[,c(7,12:14,16:26,38)], 20)
k.means.fit$centers
# k.means.fit$cluster
k.means.fit$size

kmeansPrediction <- as.data.frame(k.means.fit$centers)
kmeansPrediction <- kmeansPrediction[,5]
kmeansPrediction

closest.cluster <- function(x) {
  cluster.dist <- apply(k.means.fit$centers, 1, function(y) sqrt(sum((x-y)^2)))
  # print(c( "cluster: " ,(which.min(cluster.dist)[1]), kmeansPrediction[which.min(cluster.dist)[1]]))
  # print( kmeansPrediction[which.min(cluster.dist)[1]])
  return (kmeansPrediction[which.min(cluster.dist)[1]])
}

clusters2 <- apply(test[,c(7,12:14,16:26,38)], 1, closest.cluster)

```

```{r}
# analysis of k means clustering approach
RSFE_v1 <- test$imdb_score - clusters2
# RSFE_v1
RSFE1 <- sum(RSFE_v1)
# RSFE1
absRSFE1 <- abs(RSFE1)
absRSFE1
length(RSFE_v1)
MSFE1 <- absRSFE1 / length(RSFE_v1)
MSFE1

# calculating the mad
madtest <- mad(test$imdb_score, center = median(test$imdb_score), constant = 1)
madKmeans <- mad(clusters2, center = median(clusters2), constant = 1)
```

```{r}
#Using random forests for predicting the IMDB score

set.seed(7)
rfdf <- movieData3[sample(nrow(movieData3)), ]
rf.train <- rfdf[1:2200,]
rf.test <- rfdf[2201:nrow(rfdf), ]
paste("The dimensions of the training data are")
dim(rf.train)
paste("The dimensions of the testing data are")
dim(rf.test)
set.seed(5)

rf.rfModel <- randomForest(rfdf$imdb_score ~ rfdf$cast_total_facebook_likes + rfdf$gross + rfdf$budget + rfdf$num_critic_for_reviews +rfdf$num_user_for_reviews + rfdf$tomatoUserRating + + rfdf$tomatoRating + rfdf$tomatoReviews + rfdf$tomatoFresh + rfdf$tomatoUserMeter + rfdf$tomatoUserReviews + rfdf$duration, data = rf.train, mtry = 5)

rf.rfModel
```

```{r}
#validating the random forest model
#RMSE
rf.predictedValues <- predict(rf.rfModel, rfdf)
# rf.predictedValues


RSFE_v2 <- rfdf$imdb_score - rf.predictedValues
# RSFE_v2
RSFE2 <- sum(RSFE_v2)
# RSFE2
absRSFE2 <- abs(RSFE2)
absRSFE2
length(RSFE_v1)
MSFE2 <- absRSFE2 / length(RSFE_v2)

mean(rf.rfModel$mse)

# calculating median absolute deviation

madtestrf <- mad(rfdf$imdb_score,center = median(rfdf$imdb_score),constant = 1)
madpredictedrf <- mad(rf.predictedValues, center = median(rf.predictedValues), constant = 1)

print(c("MSFE for prediction using random forests" , MSFE2))
print(c("MSFE for prediction using K means clustering" , MSFE1))
print(c("MSFE for prediction using Linear Regression" , MSFE))

print(c("MAD for Original data:" , madtestrf))
print(c("MAD for prediction using random forests" , madpredictedrf))
print(c("MAD for Original data:" , madtestrf))
print(c("MAD for prediction using K means clustering" , madKmeans))
print(c("MAD for Original data:" , Madoriginal))
print(c("MAD for prediction using Linear Regression" , MadRegression))

```


```{r}
# Visualization of results 
visualization <- as.data.frame(c(1:6))
visualization$new <- c(1:6)
colnames(visualization) <- c("MSFE","MAD")
row.names(visualization) <- c("Original","Regression","Kmeans","Random Forest","x","y")
visualization[1,1] <- 0
visualization[2,1] <- MSFE
visualization[3,1] <- MSFE1
visualization[4,1] <- MSFE2
visualization[1,2] <- Madoriginal
visualization[2,2] <- MadRegression
visualization[3,2] <- madKmeans
visualization[4,2] <- madpredictedrf
visualization <- visualization[c(1:4),]
plot(as.factor(rownames(visualization)),visualization$MAD, xlab="Models", ylab="Calculated MAD Values", main ="original values vs Observed Values (MAD)")
axis(2,at = c(0:5))
plot(as.factor(rownames(visualization[c(1,2,4),])),visualization$MAD[c(1,2,4)], xlab="Models", ylab="Calculated MAD Values", main ="original  values vs Observed Values without Kmeans (MAD)")

# Visualizing MSFE values
plot(as.factor(rownames(visualization)),visualization$MSFE, xlab="Models", ylab="Calculated MSFE Values", main ="original values vs Observed Values (MSFE)")

plot(as.factor(rownames(visualization[c(1,2,4),])),visualization$MSFE[c(1,2,4)], xlab="Models", ylab="Calculated MSFE Values", main ="original  values vs Observed Values without Kmeans (MSFE)")

```

```{r}
paste("Looking at the MSFE values for all the three models, we can clearly see that the k-means clustering performed the worst among the three models. We can see that the Multivariate linear regression model perfomed the best and second comes random forests")

paste("order of performances: Linear Regression BETTER THAN Random Forests BETTER THAN K Means Clustering")
```


