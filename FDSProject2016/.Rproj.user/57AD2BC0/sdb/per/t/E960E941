{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Are Facebook likes, profits and movie reviews accurate predictors for IMDB score?\"\nauthor: \"kvk229 & gsc326\"\ndate: \"11/22/2016\"\noutput: pdf_document\n---\n\n\n# GAM model general additive model\n# log linear regression\n<!-- https://rstudio-pubs-static.s3.amazonaws.com/33876_1d7794d9a86647ca90c4f182df93f0e8.html -->\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n```{r echo=FALSE, include=FALSE}\n# setting up the libraries\n# require(caTools)\n#install.packages('xlsx')\nlibrary(gdata)\nlibrary(car)\nlibrary(caret)\nlibrary(caTools)\nlibrary(ggbiplot,vqv)\nlibrary(randomForest)\n#library(xlsx)\n```\n\n```{r}\n# setting the working directory\nsetwd('/Users/kkiran/Desktop/fall_2016/fds/project/MovieScorePredictor/data')\n# 'C:/Users/gogs/Documents/GitHub/MovieScorePredictor/Data'\n# setwd('C:/Users/gogs/Documents/GitHub/MovieScorePredictor/Data')\nmovieData = read.xls(\"movie_data.xls\")\n#head(movieData)\n```\n\nLoaded the data into a data frame 'movieData'\n\n```{r}\n# identifying top 10 genres out of all the 26 genres to make the work more focussed\n\n# gernes present in the data:\n# 'Sci-Fi', 'Crime', Romance', Animation', Music', Comedy', War', genres', Horror', Film-Noir', Adventure', News', Reality-TV', Thriller', Western', Mystery', Short', Drama', Action', Documentary', Musical', History', Family', Fantasy', Game-Show', Sport', Biography'\n\nmovieCount <- c()\n\nfor(i in 38:64)\n{\n  movieCount[i - 37] = sum(movieData[,i]);\n}\nmovieCount\n\ngenreNames <- as.vector(colnames(movieData)[38:64])\ngenreNames\n\ngenreNames <- as.vector(genreNames)\n\ngenreMovieCount <- data.frame(genreNames, movieCount)\nplot(genreMovieCount$genreNames, genreMovieCount$movieCount, main=\"Genre Distribution\",\txlab=\"Genre \", ylab=\" Movies Made\")\n```\n\nWe can see that not all the genres have a considerable number of movies made in them, so we decided to extract the top 11 genres that have the most number of movies made in those particular genres.\n\n```{r}\ngenreMovieCountSorted <- genreMovieCount[order(-movieCount),] \ngenreMovieCountSorted <- genreMovieCountSorted[c(1:10),]\nplot(genreMovieCountSorted$genreNames, genreMovieCountSorted$movieCount, main=\"Filtered Genre Distribution\",\txlab=\"Genre \", ylab=\" Movies Made\")\n\n```\n\nNow we have to remove the data for all the other genres from the data set, also we can delete the last column from the data set because it is repeated\n```{r}\n\nmovieData <- movieData[,-65]\ncolumnNames <- colnames(movieData)\ncolumnNames <- columnNames[1:37]\n\nselectedNames <- genreMovieCountSorted$genreNames\n\ncolumnNames <- as.vector(columnNames)\nselectedNames <- as.vector(selectedNames)\n\nnames <- c(columnNames, selectedNames)\nnames\n\nmovieData1 <- subset(movieData, select = names)\n# for(i in 38:47)\n# {\n#   print(c(colnames(movieData1[i]), sum(movieData1[,i])));\n# }\n```\nNow movieData1 has the data about the genres that we are interested only. The next step is to clean the data by removing rows that dont have a considerable amount of data. If the facebook likes are missing, although we can get those details from other row, we are not proceeding so because the number of facebook likes are always changing and fetching data from other rows might not be a very good estimate for plugging in missing likes data.\n\nNote: The facebook data is present in the data itself, we didnt have to fetch the data manually.\n\n```{r}\n\nmean(is.na(movieData))\npaste(\"only\", mean(is.na(movieData)), \" (mean) amount of data is null, so we can safely remove NAs\")\n\nrow.has.na <- apply(movieData1, 1, function(x){any(is.na(x))})\nnumberOfNAs <- sum(row.has.na)\n\nprint (c(\"can remove \", numberOfNAs , \"null rows from the table\"))\n#removing the nulls\nmovieData1 <- na.omit(movieData1)\n```\n\n```{r}\nNAcounter <- 0\nindicesToRemove <- c()\nindex <- 1\n# this is the working version \nfor (i in 1 : nrow(movieData1)) {\n  if (any(movieData1[i,] == \"N/A\")) {\n     # print (c(i, \"yes\" , movieData1[i,]))\n    indicesToRemove[index] = i;\n    index <- index + 1\n    NAcounter <- NAcounter + 1\n  } \n    # print (\"no\")\n}\n\nprint(length(indicesToRemove))\n# print(indicesToRemove)\nprint(c(\"total number of nulls\", NAcounter))\n```\n\n```{r}\n#removing the '@NAcounter' number of rows that have NA in them\nmovieData2 <- movieData1[-indicesToRemove,]\n# Now, movieData2 has no NA in any of the rows.\n\nmovieData2[movieData2==\"\"] <- NA\nrow.has.na <- apply(movieData2, 1, function(x){any(is.na(x))})\nnumberOfNAs <- sum(row.has.na)\npaste(\"There are \",numberOfNAs, \" rows with empty cells, so we are removing them\")\n# removing the empty rows, (second round of filtering)\nmovieData2 <- na.omit(movieData2)\n```\n\n\n```{r}\n#couting the profits of a movie by subtracting the budget from the gross\nmovieData2$profits <- movieData2$gross - movieData2$budget\nmovieData3 <- movieData2[,c(c(1:13),48, c(14:47))]\n\n# movieData3 is the final cleaned data that also has a column showing the profits made by the movie\n#str(movieData3)\nstat <- nearZeroVar(movieData3, saveMetrics = T)\nclass(stat$zeroVar)\nvarDF <- cbind.data.frame(colnames(movieData3),stat$zeroVar)\n\n#converting logical to binary\ncols <- sapply(varDF, is.logical)\nvarDF[,cols] <- lapply(varDF[,cols], as.numeric)\n\n# for ( i in 1:nrow(varDF)) {\n#  print(varDF[i,2]) \n# }\n\nplot(varDF$`colnames(movieData3)`, varDF$`stat$zeroVar`, xlab = \"predictors\", ylab = \"Zero - variance\", main = \"zero variance scores of different possible predictors\")\n\npaste(\"we can see that all the columns have a zero score for the zero-variance score showing that all the columns can participate as a predictor in the prediction model\")\n\n# We can see that none of the varibles have zero variance, so we can consider all the variables for study\n```\n\n\n```{r}\n\n#coverting factor to int array\nmovieData3$tomatoUserRating <- as.numeric(as.character(movieData3$tomatoUserRating))\nmovieData3$tomatoRating <- as.numeric(as.character(movieData3$tomatoRating))\nmovieData3$tomatoReviews <- as.numeric(as.character(movieData3$tomatoReviews))\nmovieData3$tomatoFresh <- as.numeric(as.character(movieData3$tomatoFresh))\nmovieData3$tomatoRotten <- as.numeric(as.character(movieData3$tomatoRotten))\nmovieData3$tomatoUserMeter <- as.numeric(as.character(movieData3$tomatoUserMeter))\nmovieData3$tomatoUserReviews <- as.numeric(as.character(movieData3$tomatoUserReviews))\nmovieData3$imdbVotes <- as.numeric(as.character(movieData3$imdbVotes))\nmovieData3$Metascore <- as.numeric(as.character(movieData3$Metascore))\n```\n\n```{r}\n# checking the correlation\n# str(movieData3)\ncor(movieData3$imdb_score, movieData3[,c(7,12:14,16:26,38)], use = \"pairwise.complete.obs\")\ncolnames(movieData3)[c(7,12:14,16:26,38)]\n```\n\n```{r}\ncNames <- paste(\"movieData3$\",colnames(movieData3)[c(7,12:14,16:26,38)], sep = \"\")\n# cNames\n#formula contains all the columns that we want to include in the model\nformula <- as.formula(paste(\"y ~ \", paste(cNames, collapse= \"+\")))\n# formula\n```\n\n\n```{r}\n#choice of linear regression vs logistic regression\n# Linear regression: When the outcome(dependent variable) is continuous, i.e. infinite number of possibilities.\n# Logistic regression: When the outcome(dependent variable) has a limited set of values.\n\n#because we are trying to predict the IMDB score of a movie and theoritically the score can have an infinite number of possibilities, we are chosing linear regression over logisitic regression\n#linear regression\n\n\nlmfit1.movieData <- lm(movieData3$imdb_score ~ movieData3$cast_total_facebook_likes + movieData3$gross + movieData3$budget + movieData3$num_critic_for_reviews +movieData3$num_user_for_reviews + movieData3$tomatoUserRating + + movieData3$tomatoRating + movieData3$tomatoReviews + movieData3$tomatoFresh + movieData3$tomatoUserMeter + movieData3$tomatoUserReviews + movieData3$num_voted_users +  movieData3$imdbVotes + movieData3$duration ,data = movieData3)\n\n# summary(lmfit1.movieData)\n\nvif(lmfit1.movieData)\n```\nLooking at the Variance Inflation Factor of the fitted model, we can see that imdbVotes and num_voted_users are two largest VIFs. so we try to eliminate them and make the model again.\n\n```{r}\n# coefficients of multi variate linear regression\npaste(\"coefficients of fitted line by linear regression\")\n# summary(lmfit1.movieData)$coefficients\n```\n\n```{r}\n lmfit2.movieData <- lm(movieData3$imdb_score ~ movieData3$cast_total_facebook_likes + movieData3$gross + movieData3$budget + movieData3$num_critic_for_reviews +movieData3$num_user_for_reviews + movieData3$tomatoUserRating + + movieData3$tomatoRating + movieData3$tomatoReviews + movieData3$tomatoFresh + movieData3$tomatoUserMeter + movieData3$tomatoUserReviews + movieData3$duration ,data = movieData3)\n\n# summary(lmfit2.movieData)\n\nlmfit2.movieData\n\nvif(lmfit2.movieData)\n# we can see that the std error for the parameter estimates gets smaller.\n```\n\n```{r}\n#calculating the MSFE and MAD for the predicted values\npredictedScore <- predict(lmfit2.movieData)\n# predictedScore\nRSFE_v <- movieData3$imdb_score - predictedScore\n# RSFE_v\nRSFE <- sum(RSFE_v)\n# RSFE\nabsRSFE <- abs(RSFE)\n# absRSFE\nlength(RSFE_v)\nMSFE <- absRSFE / length(RSFE_v)\n# MSFE\nmean(lmfit2.movieData$residuals^2)\n\n\n# calculating mad \nMadoriginal <- mad(movieData3$imdb_score,center = median(movieData3$imdb_score),constant = 1)\nMadRegression <- mad(predictedScore, center = median(predictedScore), constant = 1)\n```\n\n```{r}\n# Analysing the linear regression model\n# log.movieData <- log(movieData3[, c(7,12:26,38)])\ntrans = preProcess(x = movieData3[,c(7,12:14,16:26,38)], method=c(\"BoxCox\", \"center\", \"scale\", \"pca\"))\ntrans\ntrans$pcaComp\n\n\npca.movieData <- prcomp(x = movieData3[,c(7,12:14,16:26,38)], center = TRUE, scale. = TRUE) \nprint(pca.movieData)\ncolnames(pca.movieData$rotation)\n# summary(pca.movieData)\npca.movieData\nplot(pca.movieData, type = \"l\", main = \"PCA Movie Data\")\n```\n\n```{r}\n\nwssplot <- function(data, nc=15, seed=1234) {\n  wss <- (nrow(data)-1)*sum(apply(data,2,var))\n  bss <- (nrow(data)-1)*sum(apply(data,2,var))\n  for (i in 2:nc) {\n    set.seed(seed)\n    wss[i] <- sum(kmeans(data[,c(7,12:26,38)], centers=i)$withinss)\n    bss[i] <- sum(kmeans(data[,c(7,12:26,38)], centers=i)$betweenss)\n    }\n  \n  plot(1:nc, wss, type=\"b\", xlab=\"Number of Clusters\",\n       ylab=\"Within groups sum of squares\")\n  plot(1:nc, bss, type=\"b\", xlab=\"Number of Clusters\",\n       ylab=\"Between groups sum of squares\")\n  }\n\nwssplot(movieData3, nc=35)\npaste(\"from both of the graphs above, we can see that 20 is the optimum number of clusters because the between groups sum of squares is remaining constant after 20 clusters and similarly the within groups sum of squares is also remaining constant after 20 clusters.\")\npaste(\"The within groups sum of squares was decreasing untill 20 clusters and the between groups sum of squares was increasing untill 20 clusters\")\n```\n\n```{r}\nset.seed(101) \nsample = sample.split(movieData3, SplitRatio = .75)\ntrain = subset(movieData3, sample == TRUE)\ntest = subset(movieData3, sample == FALSE)\n```\n\n```{r}\n#using Kmeans clustering\n\npaste(\"The dimensions of the training data are\")\ndim(train)\npaste(\"The dimensions of the testing data are\")\ndim(test)\nk.means.fit <- kmeans(train[,c(7,12:14,16:26,38)], 20)\nk.means.fit$centers\n# k.means.fit$cluster\nk.means.fit$size\n\nkmeansPrediction <- as.data.frame(k.means.fit$centers)\nkmeansPrediction <- kmeansPrediction[,5]\nkmeansPrediction\n\nclosest.cluster <- function(x) {\n  cluster.dist <- apply(k.means.fit$centers, 1, function(y) sqrt(sum((x-y)^2)))\n  # print(c( \"cluster: \" ,(which.min(cluster.dist)[1]), kmeansPrediction[which.min(cluster.dist)[1]]))\n  # print( kmeansPrediction[which.min(cluster.dist)[1]])\n  return (kmeansPrediction[which.min(cluster.dist)[1]])\n}\n\nclusters2 <- apply(test[,c(7,12:14,16:26,38)], 1, closest.cluster)\n\n```\n\n```{r}\n# analysis of k means clustering approach\nRSFE_v1 <- test$imdb_score - clusters2\n# RSFE_v1\nRSFE1 <- sum(RSFE_v1)\n# RSFE1\nabsRSFE1 <- abs(RSFE1)\nabsRSFE1\nlength(RSFE_v1)\nMSFE1 <- absRSFE1 / length(RSFE_v1)\nMSFE1\n\n# calculating the mad\nmadtest <- mad(test$imdb_score, center = median(test$imdb_score), constant = 1)\nmadKmeans <- mad(clusters2, center = median(clusters2), constant = 1)\n```\n\n```{r}\n#Using random forests for predicting the IMDB score\n\nset.seed(7)\nrfdf <- movieData3[sample(nrow(movieData3)), ]\nrf.train <- rfdf[1:2200,]\nrf.test <- rfdf[2201:nrow(rfdf), ]\npaste(\"The dimensions of the training data are\")\ndim(rf.train)\npaste(\"The dimensions of the testing data are\")\ndim(rf.test)\nset.seed(5)\n\nrf.rfModel <- randomForest(rfdf$imdb_score ~ rfdf$cast_total_facebook_likes + rfdf$gross + rfdf$budget + rfdf$num_critic_for_reviews +rfdf$num_user_for_reviews + rfdf$tomatoUserRating + + rfdf$tomatoRating + rfdf$tomatoReviews + rfdf$tomatoFresh + rfdf$tomatoUserMeter + rfdf$tomatoUserReviews + rfdf$duration, data = rf.train, mtry = 5)\n\nrf.rfModel\n```\n\n```{r}\n#validating the random forest model\n#RMSE\nrf.predictedValues <- predict(rf.rfModel, rfdf)\n# rf.predictedValues\n\n\nRSFE_v2 <- rfdf$imdb_score - rf.predictedValues\n# RSFE_v2\nRSFE2 <- sum(RSFE_v2)\n# RSFE2\nabsRSFE2 <- abs(RSFE2)\nabsRSFE2\nlength(RSFE_v1)\nMSFE2 <- absRSFE2 / length(RSFE_v2)\n\nmean(rf.rfModel$mse)\n\n# calculating median absolute deviation\n\nmadtestrf <- mad(rfdf$imdb_score,center = median(rfdf$imdb_score),constant = 1)\nmadpredictedrf <- mad(rf.predictedValues, center = median(rf.predictedValues), constant = 1)\n\nprint(c(\"MSFE for prediction using random forests\" , MSFE2))\nprint(c(\"MSFE for prediction using K means clustering\" , MSFE1))\nprint(c(\"MSFE for prediction using Linear Regression\" , MSFE))\n\nprint(c(\"MAD for Original data:\" , madtestrf))\nprint(c(\"MAD for prediction using random forests\" , madpredictedrf))\nprint(c(\"MAD for Original data:\" , madtestrf))\nprint(c(\"MAD for prediction using K means clustering\" , madKmeans))\nprint(c(\"MAD for Original data:\" , Madoriginal))\nprint(c(\"MAD for prediction using Linear Regression\" , MadRegression))\n\n```\n\n\n```{r}\n# Visualization of results \nvisualization <- as.data.frame(c(1:6))\nvisualization$new <- c(1:6)\ncolnames(visualization) <- c(\"MSFE\",\"MAD\")\nrow.names(visualization) <- c(\"Original\",\"Regression\",\"Kmeans\",\"Random Forest\",\"x\",\"y\")\nvisualization[1,1] <- 0\nvisualization[2,1] <- MSFE\nvisualization[3,1] <- MSFE1\nvisualization[4,1] <- MSFE2\nvisualization[1,2] <- Madoriginal\nvisualization[2,2] <- MadRegression\nvisualization[3,2] <- madKmeans\nvisualization[4,2] <- madpredictedrf\nvisualization <- visualization[c(1:4),]\nplot(as.factor(rownames(visualization)),visualization$MAD, xlab=\"Models\", ylab=\"Calculated MAD Values\", main =\"original values vs Observed Values (MAD)\")\naxis(2,at = c(0:5))\nplot(as.factor(rownames(visualization[c(1,2,4),])),visualization$MAD[c(1,2,4)], xlab=\"Models\", ylab=\"Calculated MAD Values\", main =\"original  values vs Observed Values without Kmeans (MAD)\")\n\n# Visualizing MSFE values\nplot(as.factor(rownames(visualization)),visualization$MSFE, xlab=\"Models\", ylab=\"Calculated MSFE Values\", main =\"original values vs Observed Values (MSFE)\")\n\nplot(as.factor(rownames(visualization[c(1,2,4),])),visualization$MSFE[c(1,2,4)], xlab=\"Models\", ylab=\"Calculated MSFE Values\", main =\"original  values vs Observed Values without Kmeans (MSFE)\")\n\n```\n\n```{r}\npaste(\"Looking at the MSFE values for all the three models, we can clearly see that the k-means clustering performed the worst among the three models. We can see that the Multivariate linear regression model perfomed the best and second comes random forests\")\n\npaste(\"order of performances: Linear Regression BETTER THAN Random Forests BETTER THAN K Means Clustering\")\n```\n\n\n",
    "created" : 1481773028825.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "41871227",
    "id" : "E960E941",
    "lastKnownWriteTime" : 1481774462,
    "last_content_update" : 1481774462828,
    "path" : "~/Desktop/fall_2016/fds/project/MovieScorePredictor/FDSProject2016/projectCode.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}